# -*- coding: utf-8 -*-
"""Datapreprocess_netflix.ipynb

Before running this file download the dataset from the link in Readme.md and save the dataset in /content/netflix-prize-data directory

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KazGhfqKK6ZgiRX0aWYPaJDij0d93VZr
"""

import os
for dirname, _, filenames in os.walk('/content/netflix-prize-data'): #path of the folder where all dataset is stored
    for filename in filenames:
        print(os.path.join(dirname, filename))
import re
import csv
import pandas as pd
print("done")

def process_data(input_files, output_file):
    pattern_movie_id = r'^(\d+):\s*$'
    pattern_data = r'^(\d+),\s*([\d.]+),\s*(\d{4}-\d{2}-\d{2})$'
    record_count = 0
    with open(output_file, 'w', newline='') as w:
        writer = csv.writer(w)
        writer.writerow(['CustId', 'MovieId', 'Rating', 'Date'])

        for file in input_files:

            data = []

            movie_id = None
            #print(file)

            num = 0
            with open(file, 'r') as f:
                for line in f:
                    line = line.strip()

                    match_movie_id = re.match(pattern_movie_id, line)
                    match_data = re.match(pattern_data, line)


                    if match_movie_id:
                        movie_id = match_movie_id.group(1)
                    elif match_data:
                        cust_id = match_data.group(1)
                        rating = match_data.group(2)
                        date = match_data.group(3)
                        data.append([cust_id, movie_id, rating, date])
                        record_count = record_count + 1
                    else:
                        raise Exception('Found neither MovieId nor Data')

            writer.writerows(data)


    print("CSV file created successfully.")
    print("Records written: ", record_count)

# Example usage
input_files = ['/content/netflix_data/combined_data_1.txt', '/content/netflix_data/combined_data_2.txt', '/content/netflix_data/combined_data_3.txt', '/content/netflix_data/combined_data_1.txt']
output_file = '/content/netflix_data/Netflix_User_Ratings.csv'

process_data(input_files, output_file)

import csv
import pandas as pd

with open('/content/netflix_data/Netflix_User_Ratings.csv', mode ='r')as file:

  # reading the CSV file
  csvFile = csv.reader(file)

  count = 0
  # displaying the contents of the CSV file
  for lines in csvFile:
        print(lines)
        count = count + 1
        if count > 5:
          break

df = pd.read_csv('/content/netflix_data/Netflix_User_Ratings.csv')
print(df.shape)



# Filter sparse movies
# We can play with these variables to tune accuracy
min_movie_ratings = 10000
filter_movies = (df['MovieId'].value_counts()>min_movie_ratings)
filter_movies = filter_movies[filter_movies].index.tolist()

# Filter sparse users
min_user_ratings = 500
filter_users = (df['CustId'].value_counts()>min_user_ratings)
filter_users = filter_users[filter_users].index.tolist()

df_filtered = df[(df['MovieId'].isin(filter_movies)) & (df['CustId'].isin(filter_users))]

print(df_filtered.shape)

print("contains Nan? ", df_filtered.isnull().values.any())

